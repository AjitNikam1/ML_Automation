{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Prasanthi_sample (1).ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AjitNikam1/ML_Automation/blob/master/Task1_sample.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrOSTkc10Efm",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://shwetkm.github.io/upxlogo.png\"></img>\n",
        "\n",
        "\n",
        "# Live Project - Productionize Machine Learning models - Task 1(sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZXdcFzYIU9z",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJpvRBbYIVaF",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BmvR2YX0Efq",
        "colab_type": "code",
        "outputId": "1503a8bd-7597-43e0-b66d-508efc819cd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        }
      },
      "source": [
        "#Import basic packages\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import time\n",
        "import pandas as pd        \n",
        "import enum\n",
        "          \n",
        "import pickle\n",
        "import sys\n",
        "\n",
        "import plotly\n",
        "from plotly.data import iris\n",
        "import sklearn\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split   #splitting data\n",
        "from pylab import rcParams\n",
        "from sklearn.linear_model import LinearRegression         #linear regression\n",
        "from sklearn.metrics.regression import mean_squared_error #error metrics\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "##\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "#Code to import python module from classifier_sc.py file uploaded to your Google Drive\n",
        "!pip install pydrive                             # Package to use Google Drive API - not installed in Colab VM by default\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth                    # Other necessary packages\n",
        "from oauth2client.client import GoogleCredentials\n",
        "auth.authenticate_user()                         # Follow prompt in the authorization process\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "your_module = drive.CreateFile({\"id\": \"1c4uny6X2nv0kGR12zAOym1CMvoyDfyu-\"})   # \"your_module_file_id\" is the part after \"id=\" in the shareable link\n",
        "your_module.GetContentFile(\"classifier_sc.py\")          # Save the .py module file to Colab VM\n",
        "import classifier_sc  \n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pydrive in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from pydrive) (1.7.11)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from pydrive) (3.13)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pydrive) (4.1.3)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (1.12.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (0.0.3)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (0.11.3)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (1.4.2)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (3.0.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (0.4.7)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (0.2.7)\n",
            "Requirement already satisfied: cachetools>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->pydrive) (3.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vu1mKFF0Efw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Import dictionaries for setting basic parameters and model parameters\n",
        "from classifier_sc import classifier_config_dict,basic_params_dict,gridsearch_params_dict,MlProblemEnum,LabledDataEnum,TextDataEnum"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poDuJoLz0Ef2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Import Dataset\n",
        "df_iris = iris()\n",
        "# #Import Boston Housing dataset for testing Regression Problem\n",
        "# boston = datasets.load_boston()\n",
        "# # Convert the original array data into a dataframe and append the column names.\n",
        "# df_bston = pd.DataFrame(boston.data, columns = boston.feature_names)\n",
        "# # Add a new variable in the dataframe for the target ( or label) variable\n",
        "# df_bston['House_Price'] = boston.target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yeu7MGIHAOg2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnaUyOay0Ef7",
        "colab_type": "code",
        "outputId": "51654b16-8e56-4c2b-cf34-7a6cb36dbaf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#View columns and paste in classifier_sc.py file with relevant columns\n",
        "df_iris.columns\n",
        "# df_bston.columns\n",
        "df_iris.head()\n",
        "print(type(len(df_iris.index)))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'int'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eGaog470EgA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SpkExxK0EgI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a class to perform base model operations\n",
        "class BaseModelHelper:\n",
        "    def __init__(self,base_param,base_AlgoConfigDict,base_gridsearch_params,DataFrame,base_MlProblemEnum,base_LabledDataEnum,base_TextDataEnum):\n",
        "        self.base_param = base_param\n",
        "        self.base_AlgoConfigDict = base_AlgoConfigDict\n",
        "        print(self.base_AlgoConfigDict)\n",
        "        self.base_gridsearch_params=base_gridsearch_params\n",
        "        #Initialise DataFrame\n",
        "        self.dataframe = DataFrame\n",
        "        #Initialize X\n",
        "        self.X = self.base_param['X']\n",
        "        #Initialize y\n",
        "        self.y = self.base_param['y']\n",
        "        # self.y = OutputcolumnName \n",
        "        #set random seed\n",
        "        self.random_state = self.base_param['seed']\n",
        "        #Set test_size\n",
        "        self.test_size = self.base_param['test_size']\n",
        "        self.base_MlProblemEnum = base_MlProblemEnum\n",
        "        self.base_LabledDataEnum = base_LabledDataEnum\n",
        "        self.base_TextDataEnum = base_TextDataEnum\n",
        "        self.DataFrameCount = len(DataFrame.index)\n",
        "        print('Inside BaseModelHelper')\n",
        "        #Initialise AutoML object\n",
        "        # self.AutoMLObj = AutoML(self.dataframe,self.base_MlProblemEnum,self.base_LabledDataEnum,self.base_TextDataEnum)\n",
        "        #set base model params\n",
        "        self.base_model = self.GetMlAlgoDictionary()\n",
        "        self.base_gridsearch_params=self.base_gridsearch_params\n",
        "\n",
        "        \n",
        "    #Function to Get MlAlgoDictionary on the basis of Dataset Charactristics \n",
        "    def GetMlAlgoDictionary(self):\n",
        "    # Initialise ML Algo Dictinary\n",
        "      MlAlgoDict = dict()\n",
        "    # Check Problem type Classifiaton\n",
        "      if self.base_MlProblemEnum is MlProblemEnum.Classification:\n",
        "        #Check if have labled data or not \n",
        "        if self.base_LabledDataEnum is LabledDataEnum.Yes:\n",
        "          #Check if Dataset samples is smaller than 100K\n",
        "          if  self.DataFrameCount < 100000:\n",
        "            MlAlgoDict.update({'LinearSVC': self.base_AlgoConfigDict['LinearSVC']})\n",
        "            if self.base_TextDataEnum is TextDataEnum.Yes:\n",
        "              MlAlgoDict.update({'KNN':self.base_AlgoConfigDict['KNN'],'SVM': self.base_AlgoConfigDict['SVM'],'RandomForestClassifier':self.base_AlgoConfigDict['RandomForestClassifier']})\n",
        "              \n",
        "      return MlAlgoDict \n",
        "     \n",
        "       #Function to standardize columns\n",
        "    def normalize_columns(self):\n",
        "        \n",
        "        # X=df_iris[self.X]\n",
        "        X=self.dataframe[self.X]\n",
        "        \n",
        "        #Scale the values\n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(X)\n",
        "\n",
        "        # Scale and center the data\n",
        "        fdf_normalized = scaler.transform(X)\n",
        "\n",
        "        # Create a pandas DataFrame\n",
        "        fdf_normalized = pd.DataFrame(data=fdf_normalized, index=X.index, columns=X.columns)\n",
        "        return fdf_normalized\n",
        "    \n",
        "    #Function to perform train test split\n",
        "    def train_test_split_base(self,X_norm):\n",
        "        self.X=X_norm\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X,self.dataframe[self.y],random_state=self.random_state,test_size=self.test_size)\n",
        "    \n",
        "    #GridsearchCV parameters based on the model\n",
        "          \n",
        "    def grid_params(self,model):\n",
        "        #print(model)\n",
        "        switcher={\n",
        "            'RandomForestClassifier':self.base_gridsearch_params['RandomForestClassifier'],\n",
        "            'DecisionTreeClassifier':self.base_gridsearch_params['DecisionTreeClassifier'],\n",
        "            'LogisticRegression':self.base_gridsearch_params['LogisticRegression'],\n",
        "            'SVM':self.base_gridsearch_params['SVM'],\n",
        "            'KNN':self.base_gridsearch_params['KNN'],\n",
        "            'LinearSVC':self.base_gridsearch_params['LinearSVC']\n",
        "        }\n",
        "        return switcher.get(model, \"Invalid model\")\n",
        "        \n",
        "    \n",
        "    #Building model\n",
        "    def model_build(self):\n",
        "        \n",
        "        X_norm = self.normalize_columns()\n",
        "        b.train_test_split_base(X_norm)\n",
        "        t = PrettyTable(['Name','Train Accuracy', 'Test Accuracy', 'Parameters'])\n",
        "        t.format=False\n",
        "        for key in self.base_model:\n",
        "            model=self.base_model[key]\n",
        "            model.fit(self.X_train,self.y_train)\n",
        "            y_pred_test = model.predict(self.X_test)\n",
        "           \n",
        "                      \n",
        "            #gridsearch cv\n",
        "            random_grid = self.grid_params(str(key))\n",
        "            \n",
        "            rf_gs = GridSearchCV(model, random_grid, cv = 3, n_jobs=-1, verbose=2)\n",
        "\n",
        "            rf_gs.fit(self.X_train,self.y_train)\n",
        "            y_pred_test = rf_gs.predict(self.X_test)\n",
        "            \n",
        "            train_accuracy=accuracy_score(self.y_train, rf_gs.predict(self.X_train))*100\n",
        "            test_accuracy=accuracy_score(self.y_test, y_pred_test)*100\n",
        "           # print(\"Accuracy of GridSearchCV: {}%\".format(accuracy_score(self.y_test, y_pred_test)*100))\n",
        "            t.add_row([key,train_accuracy,test_accuracy,rf_gs.best_params_])\n",
        "            #print(t)\n",
        "        return t "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "MUfvK6Bx0EgQ",
        "colab_type": "code",
        "outputId": "54349924-e0cc-42e6-b3f8-6dfa3fbc9540",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 885
        }
      },
      "source": [
        "# Calling BaseModelHelper with basic_params_dict and classifier_config_dict from classifier_sc.py \n",
        "b=BaseModelHelper(basic_params_dict,classifier_config_dict,gridsearch_params_dict,df_iris,MlProblemEnum.Classification,LabledDataEnum.Yes,TextDataEnum.Yes)\n",
        "# Model Buildingfrom sklearn.svm import SVC \n",
        "\n",
        "t= b.model_build()\n",
        "print(t)\n",
        "Html_file= open(\"output.html\",\"w\")\n",
        "Html_file.write(t.get_html_string())\n",
        "Html_file.close()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'RandomForestClassifier': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
            "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=1, min_samples_split=2,\n",
            "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
            "                       n_jobs=None, oob_score=False, random_state=None,\n",
            "                       verbose=0, warm_start=False), 'DecisionTreeClassifier': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
            "                       max_features=None, max_leaf_nodes=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=1, min_samples_split=2,\n",
            "                       min_weight_fraction_leaf=0.0, presort=False,\n",
            "                       random_state=None, splitter='best'), 'LogisticRegression': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
            "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
            "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), 'SVM': SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
            "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
            "    kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
            "    shrinking=True, tol=0.001, verbose=False)}\n",
            "Inside BaseModelHelper\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-4139b94d244e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBaseModelHelper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasic_params_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclassifier_config_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgridsearch_params_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_iris\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMlProblemEnum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClassification\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mLabledDataEnum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mYes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTextDataEnum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mYes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# Model Buildingfrom sklearn.svm import SVC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-a0283594ab12>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, base_param, base_AlgoConfigDict, base_gridsearch_params, DataFrame, base_MlProblemEnum, base_LabledDataEnum, base_TextDataEnum)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# self.AutoMLObj = AutoML(self.dataframe,self.base_MlProblemEnum,self.base_LabledDataEnum,self.base_TextDataEnum)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m#set base model params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetMlAlgoDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_gridsearch_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_gridsearch_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-a0283594ab12>\u001b[0m in \u001b[0;36mGetMlAlgoDictionary\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     38\u001b[0m           \u001b[0;31m#Check if Dataset samples is smaller than 100K\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m           \u001b[0;32mif\u001b[0m  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrameCount\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m100000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mMlAlgoDict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'LinearSVC'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_AlgoConfigDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'LinearSVC'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_TextDataEnum\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mTextDataEnum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mYes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m               \u001b[0mMlAlgoDict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'KNN'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_AlgoConfigDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'KNN'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'SVM'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_AlgoConfigDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SVM'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'RandomForestClassifier'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_AlgoConfigDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'RandomForestClassifier'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'LinearSVC'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5x3HmjEL0EgZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1ee075c5-06ef-4b04-a948-d548734604d0"
      },
      "source": [
        "testdict = dict()\n",
        "testdict.update({'Key1':'Value1','Key2':'Value2'})\n",
        "testdict"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Key1': 'Value1', 'Key2': 'Value2'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIMHHQxQ0Egd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lva8dGFY0Egh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}